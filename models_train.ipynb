{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token : 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/pat/.cache/huggingface/token\n",
      "Login successful\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pythainlp in /Users/pat/Library/Python/3.9/lib/python/site-packages (5.0.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/pat/Library/Python/3.9/lib/python/site-packages (from pythainlp) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pat/Library/Python/3.9/lib/python/site-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pat/Library/Python/3.9/lib/python/site-packages (from requests>=2.22.0->pythainlp) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pat/Library/Python/3.9/lib/python/site-packages (from requests>=2.22.0->pythainlp) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pat/Library/Python/3.9/lib/python/site-packages (from requests>=2.22.0->pythainlp) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import torchaudio\n",
    "import matplotlib\n",
    "import timm\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()\n",
    "import glob\n",
    "import shutil\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "import torch\n",
    "set_caching_enabled(False)\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_metric,\n",
    "    set_caching_enabled)\n",
    "# from datasets.filesystems import S3FileSystem\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    Trainer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "import re\n",
    "import json\n",
    "!pip install pythainlp\n",
    "from pythainlp.tokenize import word_tokenize, syllable_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to Text (Fine Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example, tok_func = word_tokenize):\n",
    "    example['sentence'] = ' '.join(tok_func(example['sentence']))\n",
    "    return example\n",
    "\n",
    "def extract_all_chars(batch, text_col = \"sentence\"):\n",
    "    all_text = \" \".join(batch[text_col])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "def speech_file_to_array_fn(batch, \n",
    "                            text_col=\"sentence\", \n",
    "                            fname_col=\"path\",\n",
    "                            resampling_to=16000):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[fname_col])\n",
    "    resampler=torchaudio.transforms.Resample(sampling_rate, resampling_to)\n",
    "    batch[\"speech\"] = resampler(speech_array)[0].numpy()\n",
    "    batch[\"sampling_rate\"] = resampling_to\n",
    "    batch[\"target_text\"] = batch[text_col]\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(pred, processor, metric):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pat/Library/Python/3.9/lib/python/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/Users/pat/Library/Python/3.9/lib/python/site-packages/datasets/load.py:1461: FutureWarning: The repository for mozilla-foundation/common_voice_16_1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_16_1\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv_16 = load_dataset(\"mozilla-foundation/common_voice_16_1\", \"th\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  =  cv_16['train'].remove_columns(['client_id', 'path' , 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'])\n",
    "val = cv_16['validation'].remove_columns(['client_id', 'path' , 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'])\n",
    "test = cv_16['test'].remove_columns(['client_id', 'path' , 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pat/Library/Python/3.9/lib/python/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at airesearch/wav2vec2-large-xlsr-53-th were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at airesearch/wav2vec2-large-xlsr-53-th and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor , AutoModelForCTC\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"airesearch/wav2vec2-large-xlsr-53-th\")\n",
    "model = AutoModelForCTC.from_pretrained(\"airesearch/wav2vec2-large-xlsr-53-th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pat/Library/Python/3.9/lib/python/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/Users/pat/Library/Python/3.9/lib/python/site-packages/datasets/load.py:1461: FutureWarning: The repository for mozilla-foundation/common_voice_16_1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_16_1\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset....')\n",
    "datasets = load_dataset(\"mozilla-foundation/common_voice_16_1\", \"th\", use_auth_token=True)\n",
    "datasets = datasets.map(preprocess_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenization')\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"airesearch/wav2vec2-large-xlsr-53-th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature extracting....')\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, \n",
    "                                             sampling_rate=16000, \n",
    "                                             padding_value=0.0, \n",
    "                                             do_normalize=True, \n",
    "                                             return_attention_mask=False)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mapping....')\n",
    "speech_datasets = datasets.map(speech_file_to_array_fn, \n",
    "                                   remove_columns=datasets.column_names[\"train\"])\n",
    "\n",
    "\n",
    "prepared_datasets = speech_datasets.map(prepare_dataset, \n",
    "                                 remove_columns=speech_datasets.column_names[\"train\"], \n",
    "                                        batch_size=16,\n",
    "                                        batched=True)\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
